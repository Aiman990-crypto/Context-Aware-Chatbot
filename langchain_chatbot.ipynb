{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2561d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Number of chunks: 1\n",
      "ðŸ“– Sample chunk: page_content='Machine learning is a branch of artificial intelligence that focuses on the use of data and algorithms to imitate the way humans learn. \n",
      "Deep learning is a subset of machine learning that uses neural networks with many layers.\n",
      "LangChain is a framework for developing applications powered by language models. \n",
      "Retrieval-Augmented Generation (RAG) combines retrieval from a knowledge base with generative models to improve accuracy.' metadata={'source': 'knowledge.txt'}\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ðŸ“Œ Step 2: Load and Split Documents\n",
    "# ===============================\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Load your knowledge base (replace \"knowledge.txt\" with your file)\n",
    "loader = TextLoader(\"knowledge.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split into smaller chunks for embeddings\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = splitter.split_documents(docs)\n",
    "\n",
    "print(\"âœ… Number of chunks:\", len(documents))\n",
    "print(\"ðŸ“– Sample chunk:\", documents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5c59bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MP\\AppData\\Local\\Temp\\ipykernel_3896\\1034867302.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\MP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store created and saved!\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ðŸ“Œ Step 3: Create Vector Store (FAISS)\n",
    "# ===============================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Use HuggingFace embeddings wrapper (fix for compatibility)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build FAISS index\n",
    "db = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Save FAISS index locally\n",
    "db.save_local(\"vector_store\")\n",
    "\n",
    "print(\"âœ… Vector store created and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f8ac832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Local model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MP\\AppData\\Local\\Temp\\ipykernel_3896\\1126391332.py:15: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generator)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ðŸ“Œ Step 4: Load Local LLM (Flan-T5)\n",
    "# ===============================\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Use a small HuggingFace model (works on CPU)\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",   # change to \"flan-t5-small\" if slow\n",
    "    max_length=512,\n",
    "    \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "print(\"âœ… Local model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc8e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chatbot ready to use!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MP\\AppData\\Local\\Temp\\ipykernel_3896\\1772503918.py:11: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# ðŸ“Œ Step 5: Build Conversational RAG Chain\n",
    "# ===============================\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Reload FAISS with safe deserialization\n",
    "db = FAISS.load_local(\"vector_store\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Memory to keep chat history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Conversational Retrieval Chain (RAG)\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    db.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "def ask_bot(query):\n",
    "    result = qa.invoke({\"question\": query})\n",
    "\n",
    "    return result[\"answer\"]\n",
    "\n",
    "print(\"âœ… Chatbot ready to use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d44630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Chatbot is live! Type 'exit' to quit.\n",
      "\n",
      "Bot: What is LangChain?\n",
      "Bot: a framework for developing applications powered by language models\n",
      "Bot: LangChain\n",
      "Bot: Goodbye! ðŸ‘‹\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“Œ Step 6: Test the Chatbot\n",
    "# ===============================\n",
    "print(\"ðŸ¤– Chatbot is live! Type 'exit' to quit.\\n\")\n",
    "\n",
    "for _ in range(10):\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Bot: Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "    result = qa.invoke({\"question\": query})\n",
    "    print(\"Bot:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df4941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
